PYTHONPATH is /alaska/tim/Code/algorithm-reference-library/::/alaska/tim/Code/algorithm-reference-library/
Running python: /alaska/tim/anaconda3/envs/arlenv/bin/python
Running dask-scheduler: /alaska/tim/anaconda3/envs/arlenv/bin/dask-scheduler
Changed directory to /mnt/storage-ssd/tim/Code/sim-low-imaging/performance/script1/run4.

openhpc-compute-[3-10]
Working on openhpc-compute-3 ....
run dask-scheduler
run dask-worker
Working on openhpc-compute-4 ....
run dask-worker
Working on openhpc-compute-5 ....
run dask-worker
Working on openhpc-compute-6 ....
run dask-worker
Working on openhpc-compute-7 ....
run dask-worker
distributed.scheduler - INFO - -----------------------------------------------
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.5:40485'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-7fz1z7bq', purging
Working on openhpc-compute-8 ....
run dask-worker
Working on openhpc-compute-9 ....
run dask-worker
Working on openhpc-compute-10 ....
run dask-worker
Scheduler and workers now running
Scheduler is running at openhpc-compute-3
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.1:38975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.15:36942'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-ycpegum8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-sd3zjogm', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.10:45861'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-oxz57pwi', purging
distributed.scheduler - INFO - Local Directory:    /tmp/scheduler-som839il
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:   tcp://10.60.253.23:8786
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:36411
distributed.worker - INFO -          Listening to:       tcp://10.3.0.5:36411
distributed.worker - INFO -          dashboard at:             10.3.0.5:36759
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-nbg72wga
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.5:36411
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:36411
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.25:33731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.24:37042'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-8/worker-plkq04nv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-9/worker-xrwfhdqo', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.8:46451'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-7/worker-q3wy3rcu', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.17:37374'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-10/worker-sgwka9k2', purging
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.24:46042
distributed.worker - INFO -          Listening to:      tcp://10.3.0.24:46042
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.15:43929
distributed.worker - INFO -          Listening to:      tcp://10.3.0.15:43929
distributed.worker - INFO -          dashboard at:            10.3.0.15:38583
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.1:45075
distributed.worker - INFO -          Listening to:       tcp://10.3.0.1:45075
distributed.worker - INFO -          dashboard at:             10.3.0.1:34165
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -          dashboard at:            10.3.0.24:44249
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-9/worker-0vb78oz4
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.24:46042
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.24:46042
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-v70bmcup
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.15:43929
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:43929
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-ctvqmbrl
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.1:45075
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.1:45075
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.25:39320
distributed.worker - INFO -          Listening to:      tcp://10.3.0.25:39320
distributed.worker - INFO -          dashboard at:            10.3.0.25:35107
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-8/worker-w_ecxpcn
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.25:39320
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.25:39320
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.10:44172
distributed.worker - INFO -          Listening to:      tcp://10.3.0.10:44172
distributed.worker - INFO -          dashboard at:            10.3.0.10:36784
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-idcunv15
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.10:44172
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:44172
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.17:44645
distributed.worker - INFO -          Listening to:      tcp://10.3.0.17:44645
distributed.worker - INFO -          dashboard at:            10.3.0.17:34464
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-10/worker-2fsbpsph
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.17:44645
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.17:44645
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.8:32918
distributed.worker - INFO -          Listening to:       tcp://10.3.0.8:32918
distributed.worker - INFO -          dashboard at:             10.3.0.8:43206
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-7/worker-gv0dhicp
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.8:32918
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:32918
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-3:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-97b1b340-fb0b-11e9-959f-246e96489168
distributed.core - INFO - Starting established connection
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:38702 remote=tcp://10.3.0.25:39320>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:36508 remote=tcp://10.3.0.15:43929>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:54558 remote=tcp://10.3.0.10:44172>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:40294 remote=tcp://10.3.0.25:39320>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:49840 remote=tcp://10.3.0.17:44645>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:60710 remote=tcp://10.3.0.15:43929>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:58326 remote=tcp://10.3.0.8:32918>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:34866 remote=tcp://10.3.0.1:45075>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:40304 remote=tcp://10.3.0.25:39320>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:49850 remote=tcp://10.3.0.17:44645>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:60720 remote=tcp://10.3.0.15:43929>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:58336 remote=tcp://10.3.0.8:32918>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:34876 remote=tcp://10.3.0.1:45075>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:54398 remote=tcp://10.3.0.15:43929>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:59272 remote=tcp://10.3.0.25:39320>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:51550 remote=tcp://10.3.0.17:44645>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:47894 remote=tcp://10.3.0.10:44172>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 2.52 GB from 600 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 483.64 MB from 475 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 1.50 GB from 346 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 816.31 MB from 316 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 623.35 MB from 454 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 1.93 GB from 418 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 754.27 MB from 705 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 548.25 MB from 391 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 5.85 GB from 848 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 967.45 MB from 410 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 2.34 GB from 629 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 3.39 GB from 734 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 129.03 MB from 848 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 64.40 MB from 508 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 451.63 MB from 1364 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 2.13 GB from 686 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 4.72 GB from 654 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 4.19 GB from 696 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 944.64 MB from 610 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 1.03 GB from 521 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 453.98 MB from 773 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 128.99 MB from 586 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 4.19 GB from 465 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 451.37 MB from 535 reference cycles (threshold: 10.00 MB)
distributed.scheduler - INFO - Remove client Client-97b1b340-fb0b-11e9-959f-246e96489168
distributed.scheduler - INFO - Remove client Client-97b1b340-fb0b-11e9-959f-246e96489168
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/site-packages/bokeh/io/saving.py:126: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN
  warn("save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN")
distributed.scheduler - INFO - Close client connection: Client-97b1b340-fb0b-11e9-959f-246e96489168

SKA LOW imaging using ARL
Started at Wed Oct 30 11:51:21 2019

{'amplitude_loss': 0.25,
 'cellsize': None,
 'channels': [131, 139],
 'context': 'wprojectwstack',
 'deconvolve_facets': 8,
 'deconvolve_overlap': 16,
 'deconvolve_taper': 'tukey',
 'facets': 1,
 'fov': 1.4,
 'fractional_threshold': 0.2,
 'frequency_coal': 0.0,
 'local_directory': 'dask-workspace',
 'memory': 128,
 'mode': 'pipeline',
 'msname': '/mnt/storage-ssd/tim/Code/sim-low-imaging/data/EoR0_20deg_24.MS',
 'ngroup': 1,
 'niter': 1000,
 'nmajor': 3,
 'nmoment': 1,
 'npixel': None,
 'nworkers': 0,
 'nwplanes': None,
 'nwslabs': 15,
 'oversampling': 16,
 'plot': 'False',
 'restore_facets': 4,
 'serial': 'False',
 'single': 'False',
 'threads_per_worker': 1,
 'threshold': 0.01,
 'time_coal': 0.0,
 'use_serial_invert': 'True',
 'use_serial_predict': 'True',
 'weighting': 'uniform',
 'window_edge': None,
 'window_shape': None,
 'wstep': None}
Target MS is /mnt/storage-ssd/tim/Code/sim-low-imaging/data/EoR0_20deg_24.MS
[131 132 133 134 135 136 137 138 139]

Setup of processing mode
Will use dask processing
Creating Dask Client using externally defined scheduler
Diagnostic pages available on port http://10.60.253.23:8787
<Client: 'tcp://10.60.253.23:8786' processes=8 threads=8, memory=1.08 TB>
Will use serial invert
Will use serial predict

Setup of visibility ingest
[[131, 131], [132, 132], [133, 133], [134, 134], [135, 135], [136, 136], [137, 137], [138, 138]]

Setup of images
Image shape is 7776 by 7776 pixels

Setup of wide field imaging
Will do hybrid w stack/w projection, 15 w slabs, 64 w planes, support 14, w step 71.0

Setup of weighting
Will apply uniform weighting

Running pipeline
Processing took 2841.90 (s)
Quality assessment:
	Origin: qa_image
	Context: 
	Data:
		shape: '(1, 1, 7776, 7776)'
		max: '1.8553415498709644'
		min: '-0.03853544423734931'
		maxabs: '1.8553415498709644'
		rms: '0.002879426053752057'
		sum: '661.4316627858817'
		medianabs: '0.001579977104126225'
		medianabsdevmedian: '0.0015798648007894832'
		median: '-1.574176580729256e-05'

Writing restored image to EoR0_20deg_24_restored.fits
>>> Processor time used in each function
>>> read_convert 56.775 (s) 0.6 % 8 (calls)
>>> advise_wide_field 1.631 (s) 0.0 % 8 (calls)
>>> create_image_from_visibility 1.210 (s) 0.0 % 8 (calls)
>>> create_pswf_convolutionfunction 1.495 (s) 0.0 % 1 (calls)
>>> create_awterm_convolutionfunction 28.140 (s) 0.3 % 8 (calls)
>>> grid_wt 73.562 (s) 0.7 % 8 (calls)
>>> image_gather_channels 20.500 (s) 0.2 % 152 (calls)
>>> image_scatter_facets 0.192 (s) 0.0 % 36 (calls)
>>> getitem 0.014 (s) 0.0 % 1464 (calls)
>>> griddata_merge_weights 7.000 (s) 0.1 % 1 (calls)
>>> re_weight 86.094 (s) 0.8 % 8 (calls)
>>> zero 3.720 (s) 0.0 % 8 (calls)
>>> predict_list_serial_workflow 2420.610 (s) 23.7 % 40 (calls)
>>> subtract_vis 24.798 (s) 0.2 % 40 (calls)
>>> invert_list_serial_workflow 7226.693 (s) 70.7 % 48 (calls)
>>> remove_sumwt 40.457 (s) 0.4 % 10 (calls)
>>> threshold_list 2.158 (s) 0.0 % 4 (calls)
>>> deconvolve 187.797 (s) 1.8 % 144 (calls)
>>> image_gather_facets 3.387 (s) 0.0 % 4 (calls)
>>> image_scatter_channels 0.006 (s) 0.0 % 4 (calls)
>>> restore_cube 36.249 (s) 0.4 % 8 (calls)
>>> Total processor time 10222.487 (s)
>>> Total wallclock time 2857.173 (s)
>>> Speedup = 3.58

SKA LOW imaging using ARL
Started at  Wed Oct 30 11:51:21 2019
Finished at Wed Oct 30 12:40:00 2019
