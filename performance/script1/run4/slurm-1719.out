PYTHONPATH is /alaska/tim/Code/algorithm-reference-library/::/alaska/tim/Code/algorithm-reference-library/
Running python: /alaska/tim/anaconda3/envs/arlenv/bin/python
Running dask-scheduler: /alaska/tim/anaconda3/envs/arlenv/bin/dask-scheduler
Changed directory to /mnt/storage-ssd/tim/Code/sim-low-imaging/performance/script1/run4.

openhpc-compute-[0-7]
Working on openhpc-compute-0 ....
run dask-scheduler
run dask-worker
Working on openhpc-compute-1 ....
run dask-worker
Working on openhpc-compute-2 ....
run dask-worker
Working on openhpc-compute-3 ....
run dask-worker
Working on openhpc-compute-4 ....
run dask-worker
Working on openhpc-compute-5 ....
run dask-worker
distributed.scheduler - INFO - -----------------------------------------------
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.6:40756'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.6:43374'
Working on openhpc-compute-6 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-0/worker-gfpxoe2l', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.22:41360'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.22:42161'
Working on openhpc-compute-7 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-1/worker-ps51fuon', purging
Scheduler and workers now running
Scheduler is running at openhpc-compute-0
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.14:33540'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.14:36734'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-w7rrjyhq', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.5:35672'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.5:41697'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-r2kea9m6', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.15:46008'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.1:41370'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.15:37701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.1:39835'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-6zn9kjr6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-3u9l_jbo', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.10:39093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.10:42251'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-7vpbpkzj', purging
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.22:45382
distributed.worker - INFO -          Listening to:      tcp://10.3.0.22:45382
distributed.worker - INFO -          dashboard at:            10.3.0.22:46155
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-1/worker-0nki3dxv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.22:33171
distributed.worker - INFO -          Listening to:      tcp://10.3.0.22:33171
distributed.worker - INFO -          dashboard at:            10.3.0.22:42671
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-1/worker-5zmmoltg
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Local Directory:    /tmp/scheduler-ki6p_o98
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:   tcp://10.60.253.14:8786
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.scheduler - INFO - Register tcp://10.3.0.22:45382
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:45382
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.22:33171
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:33171
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.6:35672
distributed.worker - INFO -          Listening to:       tcp://10.3.0.6:35672
distributed.worker - INFO -          dashboard at:             10.3.0.6:38162
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-0/worker-69u0yskh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.6:46539
distributed.scheduler - INFO - Register tcp://10.3.0.6:35672
distributed.worker - INFO -          Listening to:       tcp://10.3.0.6:46539
distributed.worker - INFO -          dashboard at:             10.3.0.6:37336
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:35672
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-0/worker-6u1vlotw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.3.0.6:46539
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:46539
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.14:44649
distributed.worker - INFO -          Listening to:      tcp://10.3.0.14:44649
distributed.worker - INFO -          dashboard at:            10.3.0.14:46403
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-vwbocwx0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.14:34539
distributed.worker - INFO -          Listening to:      tcp://10.3.0.14:34539
distributed.worker - INFO -          dashboard at:            10.3.0.14:35877
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-yccxav2d
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.14:34539
distributed.scheduler - INFO - Register tcp://10.3.0.14:44649
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:34539
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:44649
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:44824
distributed.worker - INFO -          Listening to:       tcp://10.3.0.5:44824
distributed.worker - INFO -          dashboard at:             10.3.0.5:37197
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-0vldnuvp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:40097
distributed.worker - INFO -          Listening to:       tcp://10.3.0.5:40097
distributed.worker - INFO -          dashboard at:             10.3.0.5:33966
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-oxpm_75h
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.5:44824
distributed.scheduler - INFO - Register tcp://10.3.0.5:40097
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:44824
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:40097
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.8:44236'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.8:32829'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-7/worker-szdftuib', purging
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.1:43407
distributed.worker - INFO -          Listening to:       tcp://10.3.0.1:43407
distributed.worker - INFO -          dashboard at:             10.3.0.1:44897
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-lmk6w6j9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.1:40765
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.15:40455
distributed.worker - INFO -          Listening to:      tcp://10.3.0.15:40455
distributed.worker - INFO -          Listening to:       tcp://10.3.0.1:40765
distributed.worker - INFO -          dashboard at:             10.3.0.1:33093
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:            10.3.0.15:45939
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-qd7ubik1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.15:37756
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-fttr8p8o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:      tcp://10.3.0.15:37756
distributed.worker - INFO -          dashboard at:            10.3.0.15:39149
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-7as4ukua
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.15:40455
distributed.scheduler - INFO - Register tcp://10.3.0.1:40765
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.1:43407
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:40455
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.3.0.15:37756
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.1:40765
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.1:43407
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:37756
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.10:38779
distributed.worker - INFO -          Listening to:      tcp://10.3.0.10:38779
distributed.worker - INFO -          dashboard at:            10.3.0.10:33278
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-1rv_qvdk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.10:38295
distributed.worker - INFO -          Listening to:      tcp://10.3.0.10:38295
distributed.worker - INFO -          dashboard at:            10.3.0.10:44751
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-vhzopvh1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.10:38779
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:38779
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.3.0.10:38295
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:38295
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-f329b84a-ffbe-11e9-8564-246e964883a8
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.scheduler - INFO - Remove worker tcp://10.3.0.14:34539
distributed.core - INFO - Removing comms to tcp://10.3.0.14:34539
distributed.nanny - INFO - Worker process 24143 was killed by signal 11
distributed.scheduler - INFO - Remove worker tcp://10.3.0.1:43407
distributed.nanny - INFO - Worker process 112396 was killed by signal 11
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.core - INFO - Removing comms to tcp://10.3.0.1:43407
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - WARNING - Restarting worker
distributed.nanny - WARNING - Restarting worker
distributed.scheduler - INFO - Remove worker tcp://10.3.0.15:37756
distributed.core - INFO - Removing comms to tcp://10.3.0.15:37756
distributed.nanny - INFO - Worker process 137673 was killed by signal 11
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.14:44000
distributed.worker - INFO -          Listening to:      tcp://10.3.0.14:44000
distributed.worker - INFO -          dashboard at:            10.3.0.14:36584
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-adwdw1vk
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.14:44000
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:44000
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.1:44127
distributed.worker - INFO -          Listening to:       tcp://10.3.0.1:44127
distributed.worker - INFO -          dashboard at:             10.3.0.1:46435
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-hs0ipm3j
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.1:44127
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.1:44127
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.15:45562
distributed.worker - INFO -          Listening to:      tcp://10.3.0.15:45562
distributed.worker - INFO -          dashboard at:            10.3.0.15:41115
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-pcejh4ik
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.15:45562
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:45562
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Remove worker tcp://10.3.0.10:38295
distributed.core - INFO - Removing comms to tcp://10.3.0.10:38295
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - INFO - Worker process 120842 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
distributed.scheduler - INFO - Remove worker tcp://10.3.0.14:44649
distributed.core - INFO - Removing comms to tcp://10.3.0.14:44649
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - INFO - Worker process 24141 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
distributed.scheduler - INFO - Remove worker tcp://10.3.0.22:45382
distributed.core - INFO - Removing comms to tcp://10.3.0.22:45382
distributed.nanny - INFO - Worker process 31181 was killed by signal 11
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.10:46210
distributed.worker - INFO -          Listening to:      tcp://10.3.0.10:46210
distributed.worker - INFO -          dashboard at:            10.3.0.10:42727
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-ysyoizu3
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.10:46210
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:46210
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Remove worker tcp://10.3.0.15:40455
distributed.core - INFO - Removing comms to tcp://10.3.0.15:40455
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - INFO - Worker process 137671 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
distributed.scheduler - INFO - Remove worker tcp://10.3.0.1:40765
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - INFO - Worker process 112398 was killed by signal 11
distributed.core - INFO - Removing comms to tcp://10.3.0.1:40765
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.14:45610
distributed.worker - INFO -          Listening to:      tcp://10.3.0.14:45610
distributed.worker - INFO -          dashboard at:            10.3.0.14:46295
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-6tsy_14k
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.14:45610
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:45610
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.22:44502
distributed.worker - INFO -          Listening to:      tcp://10.3.0.22:44502
distributed.worker - INFO -          dashboard at:            10.3.0.22:46374
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-1/worker-j5hqjkfe
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.22:44502
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:44502
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.8:39671
distributed.worker - INFO -          Listening to:       tcp://10.3.0.8:39671
distributed.worker - INFO -          dashboard at:             10.3.0.8:44773
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-7/worker-lmge67nv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.8:44784
distributed.worker - INFO -          Listening to:       tcp://10.3.0.8:44784
distributed.worker - INFO -          dashboard at:             10.3.0.8:33192
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-7/worker-bmwrwyio
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.8:39671
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:39671
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.3.0.8:44784
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:44784
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.15:41585
distributed.worker - INFO -          Listening to:      tcp://10.3.0.15:41585
distributed.worker - INFO -          dashboard at:            10.3.0.15:40631
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-l7i0ibcm
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.15:41585
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:41585
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO - Worker process 120840 was killed by signal 11
distributed.scheduler - INFO - Remove worker tcp://10.3.0.10:38779
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.core - INFO - Removing comms to tcp://10.3.0.10:38779
distributed.nanny - WARNING - Restarting worker
distributed.scheduler - INFO - Remove worker tcp://10.3.0.5:40097
distributed.core - INFO - Removing comms to tcp://10.3.0.5:40097
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - INFO - Worker process 178709 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.1:35228
distributed.worker - INFO -          Listening to:       tcp://10.3.0.1:35228
distributed.worker - INFO -          dashboard at:             10.3.0.1:46151
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-x5ub0c_l
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.1:35228
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.1:35228
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.scheduler - INFO - Remove worker tcp://10.3.0.14:44000
distributed.nanny - INFO - Worker process 24299 was killed by signal 11
distributed.core - INFO - Removing comms to tcp://10.3.0.14:44000
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 137826 was killed by signal 11
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.scheduler - INFO - Remove worker tcp://10.3.0.15:45562
distributed.core - INFO - Removing comms to tcp://10.3.0.15:45562
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.10:36055
distributed.worker - INFO -          Listening to:      tcp://10.3.0.10:36055
distributed.worker - INFO -          dashboard at:            10.3.0.10:38663
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-m1zy2qdg
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.10:36055
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:36055
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:45282
distributed.worker - INFO -          Listening to:       tcp://10.3.0.5:45282
distributed.worker - INFO -          dashboard at:             10.3.0.5:35963
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-10oeus7h
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.5:45282
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:45282
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Remove worker tcp://10.3.0.1:44127
distributed.core - INFO - Removing comms to tcp://10.3.0.1:44127
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - INFO - Worker process 112550 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.14:46646
distributed.worker - INFO -          Listening to:      tcp://10.3.0.14:46646
distributed.worker - INFO -          dashboard at:            10.3.0.14:40202
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-xsuxp8b3
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.14:46646
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:46646
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Remove worker tcp://10.3.0.22:33171
distributed.core - INFO - Removing comms to tcp://10.3.0.22:33171
distributed.nanny - INFO - Worker process 31179 was killed by signal 11
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - WARNING - Restarting worker
distributed.scheduler - INFO - Remove worker tcp://10.3.0.10:46210
distributed.nanny - INFO - Worker process 120997 was killed by signal 11
distributed.core - INFO - Removing comms to tcp://10.3.0.10:46210
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.15:41763
distributed.worker - INFO -          Listening to:      tcp://10.3.0.15:41763
distributed.worker - INFO -          dashboard at:            10.3.0.15:42058
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-4ji45k0t
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.15:41763
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:41763
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.1:39701
distributed.worker - INFO -          Listening to:       tcp://10.3.0.1:39701
distributed.worker - INFO -          dashboard at:             10.3.0.1:44209
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-u38hpidw
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.1:39701
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.1:39701
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Remove worker tcp://10.3.0.14:45610
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.core - INFO - Removing comms to tcp://10.3.0.14:45610
distributed.nanny - INFO - Worker process 24374 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.22:33423
distributed.worker - INFO -          Listening to:      tcp://10.3.0.22:33423
distributed.worker - INFO -          dashboard at:            10.3.0.22:36123
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-1/worker-rwy_uja2
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.22:33423
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:33423
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.10:44304
distributed.worker - INFO -          Listening to:      tcp://10.3.0.10:44304
distributed.worker - INFO -          dashboard at:            10.3.0.10:34847
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-3ffz7lvj
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.10:44304
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:44304
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.14:36176
distributed.worker - INFO -          Listening to:      tcp://10.3.0.14:36176
distributed.worker - INFO -          dashboard at:            10.3.0.14:38376
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-lnkemaxd
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.14:36176
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:36176
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.60.253.14:46718 remote=tcp://openhpc-compute-0:8786>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:34274 remote=tcp://10.3.0.15:41763>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:34280 remote=tcp://10.3.0.15:41763>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.22:34888 remote=tcp://10.3.0.14:46646>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 2.88 GB from 26747 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:40350 remote=tcp://10.3.0.15:41763>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 483.66 MB from 28619 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:49238 remote=tcp://10.3.0.14:36176>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:43452 remote=tcp://10.3.0.5:44824>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:46828 remote=tcp://10.3.0.1:35228>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:53882 remote=tcp://10.3.0.14:46646>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:52008 remote=tcp://10.3.0.15:41585>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:52730 remote=tcp://10.3.0.6:46539>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:34442 remote=tcp://10.3.0.5:45282>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:56944 remote=tcp://10.3.0.14:36176>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:38340 remote=tcp://10.3.0.1:35228>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:56700 remote=tcp://10.3.0.14:46646>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:52188 remote=tcp://10.3.0.15:41585>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:39528 remote=tcp://10.3.0.5:44824>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:52744 remote=tcp://10.3.0.6:46539>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:34456 remote=tcp://10.3.0.5:45282>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:56958 remote=tcp://10.3.0.14:36176>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:38354 remote=tcp://10.3.0.1:35228>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:56714 remote=tcp://10.3.0.14:46646>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:52202 remote=tcp://10.3.0.15:41585>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:39542 remote=tcp://10.3.0.5:44824>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:54306 remote=tcp://10.3.0.5:45282>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:42916 remote=tcp://10.3.0.14:46646>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:58480 remote=tcp://10.3.0.15:41763>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:39592 remote=tcp://10.3.0.15:41585>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:39338 remote=tcp://10.3.0.6:46539>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:38108 remote=tcp://10.3.0.14:36176>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.1:58996 remote=tcp://10.3.0.5:44824>
distributed.utils_perf - INFO - full garbage collection released 2.60 GB from 29368 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.22:49954 remote=tcp://10.3.0.6:35672>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:35004 remote=tcp://10.3.0.5:45282>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40070 remote=tcp://10.3.0.15:41763>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:57780 remote=tcp://10.3.0.1:35228>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:45724 remote=tcp://10.3.0.15:41585>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:57454 remote=tcp://10.3.0.14:46646>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50920 remote=tcp://10.3.0.5:44824>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:54256 remote=tcp://10.3.0.10:44304>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40082 remote=tcp://10.3.0.15:41763>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:57792 remote=tcp://10.3.0.1:35228>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:45736 remote=tcp://10.3.0.15:41585>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:57466 remote=tcp://10.3.0.14:46646>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50932 remote=tcp://10.3.0.5:44824>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:54268 remote=tcp://10.3.0.10:44304>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:35030 remote=tcp://10.3.0.5:45282>
distributed.utils_perf - INFO - full garbage collection released 2.06 GB from 330 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:47210 remote=tcp://10.3.0.15:41763>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:59510 remote=tcp://10.3.0.22:44502>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:47154 remote=tcp://10.3.0.5:45282>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:41952 remote=tcp://10.3.0.14:36176>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:49516 remote=tcp://10.3.0.14:46646>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:42494 remote=tcp://10.3.0.15:41585>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:42682 remote=tcp://10.3.0.5:44824>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:59048 remote=tcp://10.3.0.6:35672>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:37504 remote=tcp://10.3.0.10:36055>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:48062 remote=tcp://10.3.0.22:33423>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:48932 remote=tcp://10.3.0.1:39701>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:45986 remote=tcp://10.3.0.10:44304>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.6:51318 remote=tcp://10.3.0.8:39671>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.6:51386 remote=tcp://10.3.0.8:39671>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 991.15 MB from 393 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 483.73 MB from 30458 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 1.02 GB from 26211 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 64.29 MB from 28459 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 612.77 MB from 982 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 3.87 GB from 439 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 34.83 GB from 29913 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 2.52 GB from 1187 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 6.77 GB from 1077 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 512.32 MB from 605 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 11.74 GB from 473 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 1.06 GB from 1135 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 5.54 GB from 553 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 1.02 GB from 920 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:42292 remote=tcp://10.3.0.8:44784>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.15:42358 remote=tcp://10.3.0.8:44784>
distributed.utils_perf - INFO - full garbage collection released 1.72 GB from 742 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 64.46 MB from 574 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 128.98 MB from 376 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 1.27 GB from 416 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 548.24 MB from 780 reference cycles (threshold: 10.00 MB)
distributed.scheduler - INFO - Remove client Client-f329b84a-ffbe-11e9-8564-246e964883a8
/alaska/tim/anaconda3/envs/arlenv/lib/python3.7/site-packages/bokeh/io/saving.py:126: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN
  warn("save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN")

SKA LOW imaging using ARL
Started at Tue Nov  5 11:25:19 2019

{'amplitude_loss': 0.25,
 'cellsize': None,
 'channels': [131, 139],
 'context': 'wprojectwstack',
 'deconvolve_facets': 8,
 'deconvolve_overlap': 16,
 'deconvolve_taper': 'tukey',
 'facets': 1,
 'fov': 1.4,
 'fractional_threshold': 0.2,
 'frequency_coal': 0.0,
 'local_directory': 'dask-workspace',
 'memory': 128,
 'mode': 'pipeline',
 'msname': '/mnt/storage-ssd/tim/Code/sim-low-imaging/data/EoR0_20deg_24.MS',
 'ngroup': 1,
 'niter': 1000,
 'nmajor': 3,
 'nmoment': 1,
 'npixel': None,
 'nworkers': 0,
 'nwplanes': None,
 'nwslabs': 15,
 'oversampling': 16,
 'plot': 'False',
 'restore_facets': 4,
 'serial': 'False',
 'single': 'False',
 'threads_per_worker': 1,
 'threshold': 0.01,
 'time_coal': 0.0,
 'use_serial_invert': 'True',
 'use_serial_predict': 'True',
 'weighting': 'uniform',
 'window_edge': None,
 'window_shape': None,
 'wstep': None}
Target MS is /mnt/storage-ssd/tim/Code/sim-low-imaging/data/EoR0_20deg_24.MS
[131 132 133 134 135 136 137 138 139]

Setup of processing mode
Will use dask processing
Creating Dask Client using externally defined scheduler
Diagnostic pages available on port http://10.60.253.14:8787
<Client: 'tcp://10.60.253.14:8786' processes=14 threads=14, memory=1.89 TB>
Will use serial invert
Will use serial predict

Setup of visibility ingest
[[131, 131], [132, 132], [133, 133], [134, 134], [135, 135], [136, 136], [137, 137], [138, 138]]

Setup of images
Image shape is 7776 by 7776 pixels

Setup of wide field imaging
Will do hybrid w stack/w projection, 15 w slabs, 64 w planes, support 14, w step 71.0

Setup of weighting
Will apply uniform weighting

Running pipeline
Processing took 2744.02 (s)
Quality assessment:
	Origin: qa_image
	Context: 
	Data:
		shape: '(1, 1, 7776, 7776)'
		max: '1.8553415498709644'
		min: '-0.03853544423734927'
		maxabs: '1.8553415498709644'
		rms: '0.002879426053752057'
		sum: '661.4316627858865'
		medianabs: '0.0015799771041263596'
		medianabsdevmedian: '0.001579864800789524'
		median: '-1.5741765807295213e-05'

Writing restored image to EoR0_20deg_24_restored.fits
>>> Processor time used in each function
>>> read_convert 24.392 (s) 0.3 % 8 (calls)
>>> advise_wide_field 1.541 (s) 0.0 % 8 (calls)
>>> create_image_from_visibility 0.972 (s) 0.0 % 8 (calls)
>>> create_pswf_convolutionfunction 1.730 (s) 0.0 % 1 (calls)
>>> create_awterm_convolutionfunction 34.914 (s) 0.5 % 8 (calls)
>>> image_gather_channels 21.657 (s) 0.3 % 152 (calls)
>>> image_scatter_facets 0.186 (s) 0.0 % 36 (calls)
>>> getitem 238.546 (s) 3.3 % 1464 (calls)
>>> grid_wt 68.206 (s) 0.9 % 8 (calls)
>>> griddata_merge_weights 5.672 (s) 0.1 % 1 (calls)
>>> re_weight 86.758 (s) 1.2 % 8 (calls)
>>> zero 1.936 (s) 0.0 % 8 (calls)
>>> invert_list_serial_workflow 5996.586 (s) 82.6 % 48 (calls)
>>> remove_sumwt 29.238 (s) 0.4 % 10 (calls)
>>> predict_list_serial_workflow 462.494 (s) 6.4 % 40 (calls)
>>> subtract_vis 38.398 (s) 0.5 % 40 (calls)
>>> threshold_list 2.269 (s) 0.0 % 4 (calls)
>>> deconvolve 202.772 (s) 2.8 % 144 (calls)
>>> image_gather_facets 2.481 (s) 0.0 % 4 (calls)
>>> image_scatter_channels 0.006 (s) 0.0 % 4 (calls)
>>> restore_cube 37.409 (s) 0.5 % 8 (calls)
>>> Total processor time 7258.165 (s)
>>> Total wallclock time 2758.866 (s)
>>> Speedup = 2.63

SKA LOW imaging using ARL
Started at  Tue Nov  5 11:25:19 2019
Finished at Tue Nov  5 12:12:20 2019
distributed.scheduler - INFO - Remove client Client-f329b84a-ffbe-11e9-8564-246e964883a8
distributed.scheduler - INFO - Close client connection: Client-f329b84a-ffbe-11e9-8564-246e964883a8
