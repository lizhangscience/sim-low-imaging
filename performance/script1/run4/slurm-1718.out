PYTHONPATH is /alaska/tim/Code/algorithm-reference-library/::/alaska/tim/Code/algorithm-reference-library/
Running python: /alaska/tim/anaconda3/envs/arlenv/bin/python
Running dask-scheduler: /alaska/tim/anaconda3/envs/arlenv/bin/dask-scheduler
Changed directory to /mnt/storage-ssd/tim/Code/sim-low-imaging/performance/script1/run4.

openhpc-compute-[0-7]
Working on openhpc-compute-0 ....
run dask-scheduler
run dask-worker
Working on openhpc-compute-1 ....
run dask-worker
Working on openhpc-compute-2 ....
run dask-worker
Working on openhpc-compute-3 ....
run dask-worker
Working on openhpc-compute-4 ....
run dask-worker
Working on openhpc-compute-5 ....
run dask-worker
distributed.scheduler - INFO - -----------------------------------------------
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.6:33606'
Working on openhpc-compute-6 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-0/worker-5kdf17ne', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.22:41193'
Working on openhpc-compute-7 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-1/worker-1wcg_w9p', purging
Scheduler and workers now running
Scheduler is running at openhpc-compute-0
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.14:34102'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-k3bp0iqd', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.5:36872'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-nfbv0h5i', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.15:37682'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.1:39194'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-kn6tuz6w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-xjig823p', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.10:37862'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-i8jn47za', purging
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.22:36674
distributed.worker - INFO -          Listening to:      tcp://10.3.0.22:36674
distributed.worker - INFO -          dashboard at:            10.3.0.22:41867
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-1/worker-ps51fuon
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Local Directory:    /tmp/scheduler-_vvechqe
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:   tcp://10.60.253.14:8786
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.scheduler - INFO - Register tcp://10.3.0.22:36674
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.22:36674
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.6:36911
distributed.worker - INFO -          Listening to:       tcp://10.3.0.6:36911
distributed.worker - INFO -          dashboard at:             10.3.0.6:40615
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-0/worker-gfpxoe2l
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.6:36911
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.6:36911
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.14:44177
distributed.worker - INFO -          Listening to:      tcp://10.3.0.14:44177
distributed.worker - INFO -          dashboard at:            10.3.0.14:40805
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-2/worker-w7rrjyhq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.5:32890
distributed.worker - INFO -          Listening to:       tcp://10.3.0.5:32890
distributed.worker - INFO -          dashboard at:             10.3.0.5:45630
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-3/worker-r2kea9m6
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.14:44177
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.14:44177
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.5:32890
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.5:32890
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.3.0.8:35016'
distributed.diskutils - INFO - Found stale lock file and directory '/mnt/storage-ssd/tim/dask-workspace/openhpc-compute-7/worker-21zve707', purging
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.15:40472
distributed.worker - INFO -          Listening to:      tcp://10.3.0.15:40472
distributed.worker - INFO -          dashboard at:            10.3.0.15:46227
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-6/worker-6zn9kjr6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.1:35013
distributed.worker - INFO -          Listening to:       tcp://10.3.0.1:35013
distributed.worker - INFO -          dashboard at:             10.3.0.1:45555
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-5/worker-3u9l_jbo
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.15:40472
distributed.scheduler - INFO - Register tcp://10.3.0.1:35013
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.15:40472
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.1:35013
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://10.3.0.10:39407
distributed.worker - INFO -          Listening to:      tcp://10.3.0.10:39407
distributed.worker - INFO -          dashboard at:            10.3.0.10:44546
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-4/worker-7vpbpkzj
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.10:39407
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.10:39407
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-70fdc9ec-ffbe-11e9-8393-246e964883a8
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO - Run out-of-band function 'init_logging'
distributed.worker - INFO -       Start worker at:       tcp://10.3.0.8:34266
distributed.worker - INFO -          Listening to:       tcp://10.3.0.8:34266
distributed.worker - INFO -          dashboard at:             10.3.0.8:44814
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  134.90 GB
distributed.worker - INFO -       Local Directory: /mnt/storage-ssd/tim/dask-workspace/openhpc-compute-7/worker-szdftuib
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.3.0.8:34266
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.3.0.8:34266
distributed.core - INFO - Starting established connection
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50710 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40694 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:52434 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50604 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:49908 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:53320 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:52472 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50754 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40738 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50646 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:49950 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:53362 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:52486 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50768 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40752 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50660 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:49964 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:53376 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:52498 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50780 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40764 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50672 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:49976 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:53388 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:52510 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50792 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40776 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50684 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:49988 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:53400 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:52522 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50804 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40788 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50696 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50000 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:53412 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:40800 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50708 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:50012 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.14:53424 remote=tcp://10.3.0.1:35013>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 754.32 MB from 30699 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.utils_perf - INFO - full garbage collection released 754.34 MB from 30701 reference cycles (threshold: 10.00 MB)
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:58056 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:51198 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:49460 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:36390 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.5:43156 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.22:54616 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.22:34852 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.22:46138 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.22:54998 remote=tcp://10.3.0.5:32890>
/alaska/tim/Code/algorithm-reference-library/processing_library/util/array_functions.py:73: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))
  weights = maskwts.dot(wts)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:48982 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:41964 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:37628 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:38060 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:43504 remote=tcp://10.3.0.6:36911>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:38476 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:57070 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:55352 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:57076 remote=tcp://10.3.0.8:34266>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:55358 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.10:38488 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:48994 remote=tcp://10.3.0.5:32890>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:41976 remote=tcp://10.3.0.1:35013>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:37640 remote=tcp://10.3.0.10:39407>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:38072 remote=tcp://10.3.0.22:36674>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.3.0.8:43516 remote=tcp://10.3.0.6:36911>
slurmstepd: error: *** JOB 1718 ON openhpc-compute-0 CANCELLED AT 2019-11-05T11:24:49 ***
Killed by signal 15.
Killed by signal 15.
Killed by signal 15.
Killed by signal 15.
Killed by signal 15.
Killed by signal 15.
Killed by signal 15.
Killed by signal 15.
Killed by signal 15.
